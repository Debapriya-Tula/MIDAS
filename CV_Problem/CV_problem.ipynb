{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ESgxb4im_Ymy"
   },
   "source": [
    "# Implementing a CNN using Keras for given task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38FfqIvG_Ym9"
   },
   "source": [
    "##### The CNN below involves a Sequential model with convolutional layers and fully connected layers with no skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "5yirTqna_YnD",
    "outputId": "c1cc6d2c-21b8-4441-c567-7e8bb4f15775"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the required modules and their sub-modules\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.models import Sequential\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have implemented a sequential model which takes (32,32,1) images as input and predicts a label. representing any of the four classes as given. I observed some of the images and found them to be blurred, but didn't apply any deblurring technique due to my ignorance about it. \n",
    "\n",
    "#### 1) I have used conv layers with ReLU activation as I found it to give a decent performance over other activations.\n",
    "#### 2) After every activation, the result is stabilized with Batch Normalization on the channel axis.\n",
    "#### 3) There are two max-pooling layers to extract relevant features and reduce the dimensionality(along with conv layers).\n",
    "#### 4) Dropout is used to ensure that the model does not overfit.\n",
    "#### 5) Dense layers occur towards the end to generate the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dBwd2Yv_Ynm"
   },
   "outputs": [],
   "source": [
    "# The CNN implemented \n",
    "def My_CNN(input_shape):\n",
    "\n",
    "  # The sequential model, a linear class of layers, is used because the model is a sequence of layers\n",
    "  model = Sequential()\n",
    "  \n",
    "  # Padding each item of the dataset with 2 columns and 2 rows of zeros for an image to be a (32,32,1) image\n",
    "  model.add(ZeroPadding2D(padding=(2,2)))\n",
    "  \n",
    "  \n",
    "  # Adding the first convolutional layer with 32 filters of size (3,3)\n",
    "  # and activation=relu, padding=same\n",
    "  model.add(Conv2D(32, kernel_size=(3,3), padding=\"same\",  activation=\"relu\", input_shape = input_shape))\n",
    "  \n",
    "  # BatchNormalization on the channel axis\n",
    "  model.add(BatchNormalization(axis=-1))\n",
    "  \n",
    "  \n",
    "  # Adding the 2nd convolutional layer with 32 filters of size (3,3)\n",
    "  # and activation=relu, padding=same\n",
    "  model.add(Conv2D(32, kernel_size=(3,3), activation=\"relu\", padding=\"same\"))\n",
    "  \n",
    "  # BatchNormalization on the channel axis\n",
    "  model.add(BatchNormalization(axis=-1))\n",
    "  \n",
    "  \n",
    "  #Adding the first max pooling layer with pool_size (2,2)\n",
    "  # and stride_size=1\n",
    "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "  \n",
    "  # Dropout to address overfitting\n",
    "  model.add(Dropout(0.25))\n",
    "            \n",
    "    \n",
    "  # Adding the 3rd convolutional layer with 64 filters of size (3,3)\n",
    "  # and activation=relu, padding=same          \n",
    "  model.add(Conv2D(64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "  \n",
    "  # BatchNormalization on the channel axis\n",
    "  model.add(BatchNormalization(axis=-1))\n",
    "  \n",
    "  \n",
    "  # Adding the 4th convolutional layer with 64 filters of size (3,3)\n",
    "  # and activation=relu, padding=same\n",
    "  model.add(Conv2D(64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "  \n",
    "  # BatchNormalization on the channel axis\n",
    "  model.add(BatchNormalization(axis=-1))\n",
    "  \n",
    "  #Adding the 2nd max pooling layer with pool_size (2,2)\n",
    "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "  \n",
    "  # Dropout to address overfitting\n",
    "  model.add(Dropout(0.25))\n",
    "            \n",
    "            \n",
    "  # Unrolling into a vector to feed into an FC layer          \n",
    "  model.add(Flatten())\n",
    "  \n",
    "  # The first layer with 512 neurons and actiation=relu\n",
    "  model.add(Dense(512, activation=\"relu\"))     \n",
    "  \n",
    "  \n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Dropout(0.5))\n",
    "  \n",
    "  # Last softmax layer for class prediction\n",
    "  model.add(Dense(4, activation=\"softmax\"))\n",
    "  \n",
    "  return model\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "ka3srIBh_Yn1",
    "outputId": "898e0c48-b424-4451-c49f-95467fec0ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " ...\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Reading the file using pickle module\n",
    "with open((\"train_image.pkl\"), 'rb') as train:\n",
    "  df = pickle.load(train)\n",
    "\n",
    "# Storing the pickled value as a numpy array\n",
    "inputs = np.array(df)\n",
    "\n",
    "# Reshaping the input images into a shape accepted by the LeNet5\n",
    "inputs = np.array([i.reshape((28, 28, 1)) for i in inputs])\n",
    "\n",
    "# Normalizing the input array\n",
    "inputs = np.multiply(inputs, 1/255.0)\n",
    "\n",
    "# Read the train_label file with pickle\n",
    "with open(os.path.abspath('train_label.pkl'), 'rb') as train_label:\n",
    "    outputs = pickle.load(train_label)\n",
    "    \n",
    "# One-hot encoding of the outputs obtained\n",
    "outputs = pd.get_dummies(pd.Series(outputs)).values\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d3ceFFrR_YoE",
    "outputId": "f85b8e7b-d66f-45bd-be58-27fb81c10bba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (8000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check whether the shape we will be feeding is right\n",
    "print(\"input_shape\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cvfu34x-_YoS"
   },
   "outputs": [],
   "source": [
    "# Call to the function with params as accepted by the input_shape parameter of a layer\n",
    "model = My_CNN((inputs.shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "ZxriA-il_Yok",
    "outputId": "247d0b3e-39d2-4287-bdee-16bd15503264"
   },
   "outputs": [],
   "source": [
    "# Configure the model with an optimization process, an objective and metric(s)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "rQJzoUvb_Yoz",
    "outputId": "1f0dfae1-f371-46e7-9333-ac889cfd2d35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 20s 3ms/step - loss: 0.6956 - acc: 0.7757 - val_loss: 2.6374 - val_acc: 0.2188TA: \n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.4610 - acc: 0.8347 - val_loss: 2.6246 - val_acc: 0.3262\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.3802 - acc: 0.8646 - val_loss: 0.8918 - val_acc: 0.6700\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.3426 - acc: 0.8762 - val_loss: 1.2183 - val_acc: 0.5637 0s - loss: 0.3417 - \n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.3094 - acc: 0.8857 - val_loss: 0.8686 - val_acc: 0.6987\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.2783 - acc: 0.8985 - val_loss: 0.7970 - val_acc: 0.7050\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.2619 - acc: 0.9050 - val_loss: 1.0553 - val_acc: 0.6162\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 14s 2ms/step - loss: 0.2548 - acc: 0.9113 - val_loss: 0.5612 - val_acc: 0.7837\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 0.2299 - acc: 0.9168 - val_loss: 0.7552 - val_acc: 0.7288\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 0.2155 - acc: 0.9239 - val_loss: 0.7338 - val_acc: 0.7338\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.2068 - acc: 0.9267 - val_loss: 0.7641 - val_acc: 0.7300\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.1942 - acc: 0.9271 - val_loss: 0.3734 - val_acc: 0.8675\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 21s 3ms/step - loss: 0.1772 - acc: 0.9368 - val_loss: 0.4675 - val_acc: 0.8450 loss: 0.1624 - ETA: 8s - loss: 0.1595 - - ETA: 7s -  - ETA: 5s - loss: 0.1695 - acc - ETA: 1s - loss: 0.17\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.1701 - acc: 0.9372 - val_loss: 0.5580 - val_acc: 0.8025\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 20s 3ms/step - loss: 0.1660 - acc: 0.9421 - val_loss: 0.6104 - val_acc: 0.7900\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.1578 - acc: 0.9422 - val_loss: 0.7002 - val_acc: 0.7600s: 0.1 - ETA: 1s - loss: 0.1569 - \n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 0.1419 - acc: 0.9465 - val_loss: 0.7422 - val_acc: 0.7738\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.1398 - acc: 0.9496 - val_loss: 0.4628 - val_acc: 0.8575\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 11s 1ms/step - loss: 0.1379 - acc: 0.9508 - val_loss: 0.8235 - val_acc: 0.7675\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 11s 1ms/step - loss: 0.1416 - acc: 0.9508 - val_loss: 0.8207 - val_acc: 0.7625\n"
     ]
    }
   ],
   "source": [
    "# Train the model and validate it over some epochs \n",
    "max_acc = 0\n",
    "\n",
    "# Storing the model trained each time for plotting at a later stage\n",
    "store_array = []\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "# Train the model and store the best model in an h5 file\n",
    "for i in range(num_epochs):\n",
    "  \n",
    "  history = model.fit(x=inputs, y=outputs, epochs=1, validation_split=0.1)\n",
    "  store_array.append([history.history['acc'], history.history['val_acc'], history.history['loss'], history.history['val_loss']])\n",
    "  \n",
    "  if history.history['val_acc'][0] > max_acc:\n",
    "    # Storing the best model \n",
    "    model.save('bestmodel.h5')\n",
    "    # Choosing only that model whose validation accuracy is the best and using it for testing\n",
    "    max_acc = history.history['val_acc'][0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "colab_type": "code",
    "id": "bX3k6Sm4_YpA",
    "outputId": "23b19e65-8476-4d92-a5ad-95ea44fc369e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum validation accuracy that could be obtained by a model: 0.8675 \n",
      "\n",
      "\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_1 (ZeroPaddin (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 2,167,524\n",
      "Trainable params: 2,166,116\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"The maximum validation accuracy that could be obtained by a model:\", max_acc, \"\\n\\n\\n\")\n",
    "\n",
    "# Displaying the parameters involved in each layer in \"My_CNN\" model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display created model\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EA3bhfy_Ypb"
   },
   "outputs": [],
   "source": [
    "# Save the model into a json file\n",
    "json_model = model.to_json()\n",
    "with open('deb_lenet5.json', 'w') as model_json:\n",
    "    model_json.write(json_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "RUK0qqno_Ypl",
    "outputId": "548ff48c-c0d8-4f9b-e309-c298dafe7029"
   },
   "outputs": [],
   "source": [
    "# Read the test file\n",
    "with open((\"test_image.pkl\"), 'rb') as test:\n",
    "  test_df = pickle.load(test)\n",
    "  \n",
    "# Store the pickled value into a numpy array\n",
    "test_inputs = np.array(test_df)\n",
    "\n",
    "# Reshaping for feeding into the network\n",
    "test_inputs = np.array([i.reshape((28, 28, 1)) for i in test_inputs])\n",
    "\n",
    "# Normalizing the input array\n",
    "test_inputs = np.multiply(inputs, 1/255.0)\n",
    "\n",
    "# Predicting the probability of an output for a given test example\n",
    "pred_proba = model.predict(test_inputs)\n",
    "\n",
    "#print(pred_proba)\n",
    "\n",
    "# Choosing the label value which bears the highest probablility\n",
    "pred_labels = np.argmax(pred_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m_2sAKkQ_Ypy",
    "outputId": "58e07785-3394-4b2b-a1e4-1f163f755c8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels of the first 10 test samples: [3 3 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "# Checking the values of the first 10 results\n",
    "print(\"Predicted labels of the first 10 test samples:\", pred_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\n"
     ]
    }
   ],
   "source": [
    "# A dictionary to map output labels obtained(one-hot encoded) to the right label\n",
    "mapper = {\n",
    "    0: 0,\n",
    "    1: 2,\n",
    "    2: 3,\n",
    "    3: 6\n",
    "}\n",
    "\n",
    "pred_labels = np.array(list(map(lambda x: mapper[x], pred_labels)))\n",
    "print(np.unique(pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "1GcrRQ6-_Yp9",
    "outputId": "922baca9-5225-4ef4-82ce-895b2e42c09c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  predicted_class\n",
      "Test_image_index                 \n",
      "0                               6\n",
      "1                               6\n",
      "2                               6\n",
      "3                               6\n",
      "4                               6\n",
      "5                               6\n",
      "6                               6\n",
      "7                               6\n",
      "8                               6\n",
      "9                               6\n"
     ]
    }
   ],
   "source": [
    "# Read the file for writing\n",
    "submission_df = pd.DataFrame(columns=['Test_image_index', 'predicted_class'])\n",
    "\n",
    "\n",
    "# Making a pandas series of the predicted labels with indexes\n",
    "pred_labels_series = pd.Series(pred_labels, dtype='int32')\n",
    "submission_df['predicted_class'] = pred_labels_series\n",
    "submission_df['Test_image_index'] = pd.Series(list(range(len(pred_labels_series))))\n",
    "\n",
    "# Identifying the 'Test_image_index' column as the index column\n",
    "submission_df = submission_df.set_index(['Test_image_index'])\n",
    "\n",
    "# displaying the first 20 predictions\n",
    "print(submission_df.head(10))\n",
    "\n",
    "# Saving to file \"sample_submission.csv\"\n",
    "submission_df.to_csv(os.path.abspath('DebapriyaTula.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "colab_type": "code",
    "id": "Juo90v8AD6Jk",
    "outputId": "01d0ea74-da0c-4e19-f6df-5c049470c98d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For accuracy evaluation on train and test examples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(store_array[:][0])\n",
    "plt.plot(store_array[:][1])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(store_array[:][2])\n",
    "plt.plot(store_array[:][3])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deb_iiit_submit.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
